{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from sklearn.model_selection import train_test_split\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "os.chdir(\"C:\\\\Users\\\\jleus\\\\PycharmProjects\\\\APLResearch\\\\Ice Data\\\\networks\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [],
   "source": [
    "cmap_colors = [(0.00, 0.0, 0.0), (1.0, 1.0, 1.0)] # White to Grey\n",
    "cmap_name = 'custom_colormap'\n",
    "cmapGrey = LinearSegmentedColormap.from_list(cmap_name, cmap_colors)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [],
   "source": [
    "def read_file(date):\n",
    "    mapDf = pd.read_csv('C:\\\\Users\\\\jleus\\\\PycharmProjects\\\\APLResearch\\\\Ice Data\\\\SSMR\\\\iceDataframes\\\\maps\\\\' + date + 'map.csv')\n",
    "    mapDf = mapDf.drop(['Unnamed: 0'], axis=1)\n",
    "    outlineDf = pd.read_csv('C:\\\\Users\\\\jleus\\\\PycharmProjects\\\\APLResearch\\\\Ice Data\\\\SSMR\\\\iceDataframes\\\\oceans\\\\' + date + 'oceans.csv')\n",
    "    outlineDf = outlineDf.drop(['Unnamed: 0'], axis=1)\n",
    "    return mapDf, outlineDf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [],
   "source": [
    "def prep_images():\n",
    "    os.chdir('C:\\\\Users\\\\jleus\\\\PycharmProjects\\\\APLResearch\\\\Ice Data\\\\SSMR\\\\iceDataframes\\\\oceans')\n",
    "    fnames = os.listdir()\n",
    "    dates = [file[:8] for file in fnames]\n",
    "    i=0\n",
    "    for date in dates:\n",
    "        try:\n",
    "            mapDf, outlineDf = read_file(date)\n",
    "            os.chdir('C:\\\\Users\\\\jleus\\\\PycharmProjects\\\\APLResearch\\\\Ice Data\\\\networks\\\\CNNimages')\n",
    "            fig, ax = plt.subplots(figsize=(304/77, 448/77))\n",
    "            im1 = ax.imshow(mapDf, cmap='jet_r')\n",
    "            im2 = ax.imshow(outlineDf, cmap=cmapGrey, alpha=1)\n",
    "            plt.axis('off')\n",
    "            plt.savefig(f'{date}_CNN.png', bbox_inches='tight', pad_inches=0)\n",
    "            plt.close()\n",
    "        except:\n",
    "            print(f'Failed: {date}')\n",
    "        i+=1\n",
    "        if (i%100) == 0:\n",
    "            print(f'Done with: {i}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "def extract_images():\n",
    "    imgsPaths = []\n",
    "    dates = []\n",
    "    os.chdir('C:\\\\Users\\\\jleus\\\\PycharmProjects\\\\APLResearch\\\\Ice Data\\\\networks\\\\CNNimages')\n",
    "    fnames = os.listdir()\n",
    "    i=0\n",
    "    print(len(fnames))\n",
    "    for file in fnames:\n",
    "        try:\n",
    "            dates.append(pd.to_datetime(file[4:8], format='%m%d'))\n",
    "            imgsPaths.append(f'C:\\\\Users\\\\jleus\\\\PycharmProjects\\\\APLResearch\\\\Ice Data\\\\networks\\\\CNNimages\\\\{file}')\n",
    "            if (i%100) == 0:\n",
    "                print(f'Done with: {i}')\n",
    "            i+=1\n",
    "        except:\n",
    "            pass\n",
    "    df = pd.DataFrame({'Date': dates, 'ImagePath': imgsPaths})\n",
    "    df['DayOfYearSin'] = np.sin(2*np.pi*df['Date'].dt.dayofyear/365)\n",
    "    df['DayOfYearCos'] = np.cos(2*np.pi*df['Date'].dt.dayofyear/365)\n",
    "    return df"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "def preprocess_image(path):\n",
    "    img = cv2.imread(path)\n",
    "    img = img / 255.0\n",
    "    return img"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "def load_images(image_paths):\n",
    "    images = []\n",
    "    for path in image_paths:\n",
    "        img = cv2.imread(path)\n",
    "        img = np.array(img)\n",
    "        images.append(img)\n",
    "    return np.array(images)/255"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df = extract_images()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), input_shape=(448, 304, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Flatten(),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dense(2),\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['ImagePath'].values,\n",
    "                                                        df[['DayOfYearSin', 'DayOfYearCos']].values,\n",
    "                                                        test_size=0.2, random_state=42)\n",
    "    X_train = load_images(X_train)\n",
    "    X_test = load_images(X_test)\n",
    "\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)\n",
    "    return history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1507\n",
      "Done with: 0\n",
      "Done with: 100\n",
      "Done with: 200\n",
      "Done with: 300\n",
      "Done with: 400\n",
      "Done with: 500\n",
      "Done with: 600\n",
      "Done with: 700\n",
      "Done with: 800\n",
      "Done with: 900\n",
      "Done with: 1000\n",
      "Done with: 1100\n",
      "Done with: 1200\n",
      "Done with: 1300\n",
      "Done with: 1400\n"
     ]
    }
   ],
   "source": [
    "df = extract_images()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "37/37 [==============================] - 49s 1s/step - loss: -644596736.0000 - accuracy: 0.5973 - val_loss: -720168704.0000 - val_accuracy: 0.5445\n",
      "Epoch 2/10\n",
      "37/37 [==============================] - 45s 1s/step - loss: -773842560.0000 - accuracy: 0.6041 - val_loss: -866834624.0000 - val_accuracy: 0.5342\n",
      "Epoch 3/10\n",
      "37/37 [==============================] - 45s 1s/step - loss: -931347840.0000 - accuracy: 0.5981 - val_loss: -1042969984.0000 - val_accuracy: 0.5514\n",
      "Epoch 4/10\n",
      "37/37 [==============================] - 49s 1s/step - loss: -1118358144.0000 - accuracy: 0.6084 - val_loss: -1262646400.0000 - val_accuracy: 0.5548\n",
      "Epoch 5/10\n",
      "37/37 [==============================] - 64s 2s/step - loss: -1351138944.0000 - accuracy: 0.5973 - val_loss: -1527923968.0000 - val_accuracy: 0.5616\n",
      "Epoch 6/10\n",
      "37/37 [==============================] - 81s 2s/step - loss: -1638620544.0000 - accuracy: 0.5964 - val_loss: -1847288320.0000 - val_accuracy: 0.5616\n",
      "Epoch 7/10\n",
      "37/37 [==============================] - 62s 2s/step - loss: -1986745600.0000 - accuracy: 0.5981 - val_loss: -2248002304.0000 - val_accuracy: 0.5651\n",
      "Epoch 8/10\n",
      "37/37 [==============================] - 61s 2s/step - loss: -2407377664.0000 - accuracy: 0.5835 - val_loss: -2733045248.0000 - val_accuracy: 0.5925\n",
      "Epoch 9/10\n",
      "37/37 [==============================] - 76s 2s/step - loss: -2908749568.0000 - accuracy: 0.5750 - val_loss: -3308781056.0000 - val_accuracy: 0.6062\n",
      "Epoch 10/10\n",
      "37/37 [==============================] - 53s 1s/step - loss: -3514468352.0000 - accuracy: 0.5733 - val_loss: -3995175936.0000 - val_accuracy: 0.6233\n"
     ]
    }
   ],
   "source": [
    "train_model(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import matplotlib.pyplot as plt\n",
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from tensorflow.keras import Sequential\n",
      "from tensorflow.keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense\n",
      "import torch\n",
      "import os\n",
      "import cv2\n",
      "from matplotlib.colors import LinearSegmentedColormap\n",
      "os.chdir(\"C:\\\\Users\\\\jleus\\\\PycharmProjects\\\\APLResearch\\\\Ice Data\\\\networks\")\n",
      "cmap_colors = [(0.00, 0.0, 0.0), (1.0, 1.0, 1.0)] # White to Grey\n",
      "cmap_name = 'custom_colormap'\n",
      "cmapGrey = LinearSegmentedColormap.from_list(cmap_name, cmap_colors)\n",
      "def read_file(date):\n",
      "    mapDf = pd.read_csv('C:\\\\Users\\\\jleus\\\\PycharmProjects\\\\APLResearch\\\\Ice Data\\\\SSMR\\\\iceDataframes\\\\maps\\\\' + date + 'map.csv')\n",
      "    mapDf = mapDf.drop(['Unnamed: 0'], axis=1)\n",
      "    outlineDf = pd.read_csv('C:\\\\Users\\\\jleus\\\\PycharmProjects\\\\APLResearch\\\\Ice Data\\\\SSMR\\\\iceDataframes\\\\oceans\\\\' + date + 'oceans.csv')\n",
      "    outlineDf = outlineDf.drop(['Unnamed: 0'], axis=1)\n",
      "    return mapDf, outlineDf\n",
      "def prep_images():\n",
      "    os.chdir('C:\\\\Users\\\\jleus\\\\PycharmProjects\\\\APLResearch\\\\Ice Data\\\\SSMR\\\\iceDataframes\\\\oceans')\n",
      "    fnames = os.listdir()\n",
      "    dates = [file[:8] for file in fnames]\n",
      "    i=0\n",
      "    for date in dates:\n",
      "        try:\n",
      "            mapDf, outlineDf = read_file(date)\n",
      "            os.chdir('C:\\\\Users\\\\jleus\\\\PycharmProjects\\\\APLResearch\\\\Ice Data\\\\networks\\\\CNNimages')\n",
      "            fig, ax = plt.subplots(figsize=(304/77, 448/77))\n",
      "            im1 = ax.imshow(mapDf, cmap='jet_r')\n",
      "            im2 = ax.imshow(outlineDf, cmap=cmapGrey, alpha=1)\n",
      "            plt.axis('off')\n",
      "            plt.savefig(f'{date}_CNN.png', bbox_inches='tight', pad_inches=0)\n",
      "            plt.close()\n",
      "        except:\n",
      "            print(f'Failed: {date}')\n",
      "        i+=1\n",
      "        if (i%100) == 0:\n",
      "            print(f'Done with: {i}')\n",
      "def extract_images():\n",
      "    imgsPaths = []\n",
      "    dates = []\n",
      "    os.chdir('C:\\\\Users\\\\jleus\\\\PycharmProjects\\\\APLResearch\\\\Ice Data\\\\networks\\\\CNNimages')\n",
      "    fnames = os.listdir()\n",
      "    i=0\n",
      "    print(len(fnames))\n",
      "    for file in fnames:\n",
      "        try:\n",
      "            dates.append(pd.to_datetime(file[4:8], format='%m%d'))\n",
      "            imgsPaths.append(f'C:\\\\Users\\\\jleus\\\\PycharmProjects\\\\APLResearch\\\\Ice Data\\\\networks\\\\CNNimages\\\\{file}')\n",
      "            if (i%100) == 0:\n",
      "                print(f'Done with: {i}')\n",
      "            i+=1\n",
      "        except:\n",
      "            pass\n",
      "    df = pd.DataFrame({'Date': dates, 'ImagePath': imgsPaths})\n",
      "    df['DayOfYearSin'] = np.sin(2*np.pi*df['Date'].dt.dayofyear/365)\n",
      "    df['DayOfYearCos'] = np.cos(2*np.pi*df['Date'].dt.dayofyear/365)\n",
      "    return df\n",
      "df = extract_images()\n",
      "model = Sequential([\n",
      "    Conv2D(32, (3, 3), input_shape=(448, 304, 3), activation='relu'),\n",
      "    MaxPooling2D(pool_size=(2, 2)),\n",
      "    Conv2D(64, (3, 3), activation='relu'),\n",
      "    MaxPooling2D(pool_size=(2, 2)),\n",
      "    Flatten(),\n",
      "    Dense(128, activation='relu'),\n",
      "    Dense(2),\n",
      "])\n",
      "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
      "              optimizer='adam',\n",
      "              metrics=['accuracy'])\n",
      "image_paths = df['ImagePath'].values\n",
      "image_paths\n",
      "print(image_paths)\n",
      "image_paths = df['ImagePath'].values\n",
      "image_labels = df[['DayOfYearSin', 'DayOfYearCos']].values\n",
      "print(image_labels)\n",
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from tensorflow.keras import Sequential\n",
      "from tensorflow.keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense\n",
      "import torch\n",
      "import os\n",
      "import cv2\n",
      "from matplotlib.colors import LinearSegmentedColormap\n",
      "from sklearn.model_selection import train_test_split\n",
      "X_train, X_test, y_train, y_test = train_test_split(image_paths, image_labels, test_size=0.2, random_state=42)\n",
      "len(X_train)\n",
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from tensorflow.keras import Sequential\n",
      "from tensorflow.keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense\n",
      "import torch\n",
      "import os\n",
      "import cv2\n",
      "from matplotlib.colors import LinearSegmentedColormap\n",
      "from sklearn.model_selection import train_test_split\n",
      "from keras.preprocessing.image import load_img, img_to_array\n",
      "df['ImagePath'][0]\n",
      "img = cv2.imread(df['ImagePath'][0])\n",
      "img\n",
      "img[0]\n",
      "img\n",
      "img.shape\n",
      "img = np.expand_dims(img, axis=0)\n",
      "img.shape\n",
      "img = cv2.imread(df['ImagePath'][0])\n",
      "img = img / 255.0\n",
      "img\n",
      "img[0]\n",
      "plt.imshow(img)\n",
      "def generate_batches(paths, labels, batch_size):\n",
      "    while True:\n",
      "        for i in range(0, len(paths), batch_size):\n",
      "            image_batch = [preprocess_image(path) for path in paths[i:i+batch_size]]\n",
      "            label_batch = labels[i:i+batch_size]\n",
      "            yield np.concatenate(image_batch, axis=0), label_batch\n",
      "model = Sequential([\n",
      "    Conv2D(32, (3, 3), input_shape=(448, 304, 3), activation='relu'),\n",
      "    MaxPooling2D(pool_size=(2, 2)),\n",
      "    Conv2D(64, (3, 3), activation='relu'),\n",
      "    MaxPooling2D(pool_size=(2, 2)),\n",
      "    Flatten(),\n",
      "    Dense(128, activation='relu'),\n",
      "    Dense(2),\n",
      "])\n",
      "train_generator = generate_batches(X_train, y_train, batch_size=32)\n",
      "test_generator = generate_batches(X_test, y_test, batch_size=32)\n",
      "model.fit(train_generator, ysteps_per_epoch=len(X_train)//32, validation_data=test_generator, validation_steps=len(X_test)//32, epochs=10)\n",
      "model.fit(train_generator, steps_per_epoch=len(X_train)//32, validation_data=test_generator, validation_steps=len(X_test)//32, epochs=10)\n",
      "model = Sequential([\n",
      "    Conv2D(32, (3, 3), input_shape=(448, 304, 3), activation='relu'),\n",
      "    MaxPooling2D(pool_size=(2, 2)),\n",
      "    Conv2D(64, (3, 3), activation='relu'),\n",
      "    MaxPooling2D(pool_size=(2, 2)),\n",
      "    Flatten(),\n",
      "    Dense(128, activation='relu'),\n",
      "    Dense(2),\n",
      "])\n",
      "model = Sequential([\n",
      "    Conv2D(32, (3, 3), input_shape=(448, 304, 3), activation='relu'),\n",
      "    MaxPooling2D(pool_size=(2, 2)),\n",
      "    Conv2D(64, (3, 3), activation='relu'),\n",
      "    MaxPooling2D(pool_size=(2, 2)),\n",
      "    Flatten(),\n",
      "    Dense(128, activation='relu'),\n",
      "    Dense(2),\n",
      "])\n",
      "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
      "              optimizer='adam',\n",
      "              metrics=['accuracy'])\n",
      "image_paths = df['ImagePath'].values\n",
      "image_labels = df[['DayOfYearSin', 'DayOfYearCos']].values\n",
      "X_train, X_test, y_train, y_test = train_test_split(image_paths, image_labels, test_size=0.2, random_state=42)\n",
      "train_generator = generate_batches(X_train, y_train, batch_size=32)\n",
      "test_generator = generate_batches(X_test, y_test, batch_size=32)\n",
      "model.fit(train_generator, steps_per_epoch=len(X_train)//32, validation_data=test_generator, validation_steps=len(X_test)//32, epochs=10)\n",
      "import matplotlib.pyplot as plt\n",
      "import tensorflow as tf\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "from tensorflow.keras import Sequential\n",
      "from tensorflow.keras.layers import TimeDistributed, Conv2D, MaxPooling2D, Flatten, LSTM, Dense\n",
      "import torch\n",
      "import os\n",
      "import cv2\n",
      "from matplotlib.colors import LinearSegmentedColormap\n",
      "from sklearn.model_selection import train_test_split\n",
      "cmap_colors = [(0.00, 0.0, 0.0), (1.0, 1.0, 1.0)] # White to Grey\n",
      "cmap_name = 'custom_colormap'\n",
      "cmapGrey = LinearSegmentedColormap.from_list(cmap_name, cmap_colors)\n",
      "def read_file(date):\n",
      "    mapDf = pd.read_csv('C:\\\\Users\\\\jleus\\\\PycharmProjects\\\\APLResearch\\\\Ice Data\\\\SSMR\\\\iceDataframes\\\\maps\\\\' + date + 'map.csv')\n",
      "    mapDf = mapDf.drop(['Unnamed: 0'], axis=1)\n",
      "    outlineDf = pd.read_csv('C:\\\\Users\\\\jleus\\\\PycharmProjects\\\\APLResearch\\\\Ice Data\\\\SSMR\\\\iceDataframes\\\\oceans\\\\' + date + 'oceans.csv')\n",
      "    outlineDf = outlineDf.drop(['Unnamed: 0'], axis=1)\n",
      "    return mapDf, outlineDf\n",
      "def prep_images():\n",
      "    os.chdir('C:\\\\Users\\\\jleus\\\\PycharmProjects\\\\APLResearch\\\\Ice Data\\\\SSMR\\\\iceDataframes\\\\oceans')\n",
      "    fnames = os.listdir()\n",
      "    dates = [file[:8] for file in fnames]\n",
      "    i=0\n",
      "    for date in dates:\n",
      "        try:\n",
      "            mapDf, outlineDf = read_file(date)\n",
      "            os.chdir('C:\\\\Users\\\\jleus\\\\PycharmProjects\\\\APLResearch\\\\Ice Data\\\\networks\\\\CNNimages')\n",
      "            fig, ax = plt.subplots(figsize=(304/77, 448/77))\n",
      "            im1 = ax.imshow(mapDf, cmap='jet_r')\n",
      "            im2 = ax.imshow(outlineDf, cmap=cmapGrey, alpha=1)\n",
      "            plt.axis('off')\n",
      "            plt.savefig(f'{date}_CNN.png', bbox_inches='tight', pad_inches=0)\n",
      "            plt.close()\n",
      "        except:\n",
      "            print(f'Failed: {date}')\n",
      "        i+=1\n",
      "        if (i%100) == 0:\n",
      "            print(f'Done with: {i}')\n",
      "def extract_images():\n",
      "    imgsPaths = []\n",
      "    dates = []\n",
      "    os.chdir('C:\\\\Users\\\\jleus\\\\PycharmProjects\\\\APLResearch\\\\Ice Data\\\\networks\\\\CNNimages')\n",
      "    fnames = os.listdir()\n",
      "    i=0\n",
      "    print(len(fnames))\n",
      "    for file in fnames:\n",
      "        try:\n",
      "            dates.append(pd.to_datetime(file[4:8], format='%m%d'))\n",
      "            imgsPaths.append(f'C:\\\\Users\\\\jleus\\\\PycharmProjects\\\\APLResearch\\\\Ice Data\\\\networks\\\\CNNimages\\\\{file}')\n",
      "            if (i%100) == 0:\n",
      "                print(f'Done with: {i}')\n",
      "            i+=1\n",
      "        except:\n",
      "            pass\n",
      "    df = pd.DataFrame({'Date': dates, 'ImagePath': imgsPaths})\n",
      "    df['DayOfYearSin'] = np.sin(2*np.pi*df['Date'].dt.dayofyear/365)\n",
      "    df['DayOfYearCos'] = np.cos(2*np.pi*df['Date'].dt.dayofyear/365)\n",
      "    return df\n",
      "def preprocess_image(path):\n",
      "    img = cv2.imload(path)\n",
      "    img /= 255.0\n",
      "    return img\n",
      "def generate_batches(paths, labels, batch_size):\n",
      "    while True:\n",
      "        for i in range(0, len(paths), batch_size):\n",
      "            image_batch = [preprocess_image(path) for path in paths[i:i+batch_size]]\n",
      "            label_batch = labels[i:i+batch_size]\n",
      "            yield np.concatenate(image_batch, axis=0), label_batch\n",
      "df = extract_images()\n",
      "model = Sequential([\n",
      "    Conv2D(32, (3, 3), input_shape=(448, 304, 3), activation='relu'),\n",
      "    MaxPooling2D(pool_size=(2, 2)),\n",
      "    Conv2D(64, (3, 3), activation='relu'),\n",
      "    MaxPooling2D(pool_size=(2, 2)),\n",
      "    Flatten(),\n",
      "    Dense(128, activation='relu'),\n",
      "    Dense(2),\n",
      "])\n",
      "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
      "              optimizer='adam',\n",
      "              metrics=['accuracy'])\n",
      "image_paths = df['ImagePath'].values\n",
      "image_labels = df[['DayOfYearSin', 'DayOfYearCos']].values\n",
      "X_train, X_test, y_train, y_test = train_test_split(image_paths, image_labels, test_size=0.2, random_state=42)\n",
      "train_generator = generate_batches(X_train, y_train, batch_size=32)\n",
      "test_generator = generate_batches(X_test, y_test, batch_size=32)\n",
      "model.fit(train_generator, steps_per_epoch=len(X_train)//32, validation_data=test_generator, validation_steps=len(X_test)//32, epochs=10)\n",
      "def preprocess_image(path):\n",
      "    img = cv2.imread(path)\n",
      "    img /= 255.0\n",
      "    return img\n",
      "def generate_batches(paths, labels, batch_size):\n",
      "    while True:\n",
      "        for i in range(0, len(paths), batch_size):\n",
      "            image_batch = [preprocess_image(path) for path in paths[i:i+batch_size]]\n",
      "            label_batch = labels[i:i+batch_size]\n",
      "            yield np.concatenate(image_batch, axis=0), label_batch\n",
      "df = extract_images()\n",
      "model = Sequential([\n",
      "    Conv2D(32, (3, 3), input_shape=(448, 304, 3), activation='relu'),\n",
      "    MaxPooling2D(pool_size=(2, 2)),\n",
      "    Conv2D(64, (3, 3), activation='relu'),\n",
      "    MaxPooling2D(pool_size=(2, 2)),\n",
      "    Flatten(),\n",
      "    Dense(128, activation='relu'),\n",
      "    Dense(2),\n",
      "])\n",
      "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
      "              optimizer='adam',\n",
      "              metrics=['accuracy'])\n",
      "image_paths = df['ImagePath'].values\n",
      "image_labels = df[['DayOfYearSin', 'DayOfYearCos']].values\n",
      "image_paths = df['ImagePath'].values\n",
      "image_labels = df[['DayOfYearSin', 'DayOfYearCos']].values\n",
      "X_train, X_test, y_train, y_test = train_test_split(image_paths, image_labels, test_size=0.2, random_state=42)\n",
      "X_train, X_test, y_train, y_test = train_test_split(image_paths, image_labels, test_size=0.2, random_state=42)\n",
      "train_generator = generate_batches(X_train, y_train, batch_size=32)\n",
      "test_generator = generate_batches(X_test, y_test, batch_size=32)\n",
      "model.fit(train_generator, steps_per_epoch=len(X_train)//32, validation_data=test_generator, validation_steps=len(X_test)//32, epochs=10)\n",
      "def preprocess_image(path):\n",
      "    img = cv2.imread(path)\n",
      "    img = img / 255.0\n",
      "    return img\n",
      "model.fit(train_generator, steps_per_epoch=len(X_train)//32, validation_data=test_generator, validation_steps=len(X_test)//32, epochs=10)\n",
      "def load_images(image_paths):\n",
      "    images = []\n",
      "    for path in image_paths:\n",
      "        img = cv2.imread(path)\n",
      "        img = np.array(img)\n",
      "        images.append(img)\n",
      "    return np.array(images)/255\n",
      "def train_model(model):\n",
      "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
      "              optimizer='adam',\n",
      "              metrics=['accuracy'])\n",
      "    X_train, X_test, y_train, y_test = train_test_split(df['ImagePath'].values,\n",
      "                                                        df[['DayOfYearSin', 'DayOfYearCos']].values, \n",
      "                                                        test_size=0.2, random_state=42)\n",
      "    X_train = load_images(X_train)\n",
      "    X_test = load_images(X_test)\n",
      "    \n",
      "    model.fit(X_train, y_train, validation_date=(X_test, y_test), epochs=10)\n",
      "df = extract_images()\n",
      "train_model()\n",
      "train_model(model)\n",
      "def train_model(model):\n",
      "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
      "              optimizer='adam',\n",
      "              metrics=['accuracy'])\n",
      "    X_train, X_test, y_train, y_test = train_test_split(df['ImagePath'].values,\n",
      "                                                        df[['DayOfYearSin', 'DayOfYearCos']].values,\n",
      "                                                        test_size=0.2, random_state=42)\n",
      "    X_train = load_images(X_train)\n",
      "    X_test = load_images(X_test)\n",
      "\n",
      "    model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)\n",
      "df = extract_images()\n",
      "train_model(model, verbose=)\n",
      "train_model(model, verbose=True)\n",
      "train_model(model)\n",
      "def train_model(model):\n",
      "    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
      "              optimizer='adam',\n",
      "              metrics=['accuracy'])\n",
      "    X_train, X_test, y_train, y_test = train_test_split(df['ImagePath'].values,\n",
      "                                                        df[['DayOfYearSin', 'DayOfYearCos']].values,\n",
      "                                                        test_size=0.2, random_state=42)\n",
      "    X_train = load_images(X_train)\n",
      "    X_test = load_images(X_test)\n",
      "\n",
      "    history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=10)\n",
      "df = extract_images()\n",
      "train_model(model)\n",
      "history\n"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}